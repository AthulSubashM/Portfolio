<!DOCTYPE html>
<html>
    <head>
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <title>Assignment 3</title>
        <link type="text/css" rel="stylesheet" href="main.css"/>
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
        
    </head>


    <body>
        
        <nav class="navbar" id="navbar">
            <ul class="menu">
                    <li><a href="index.html">Home</a></li>
                    <li><a href="assignment1.html">Assignment 1</a></li>
                    <li><a href="assignment2.html">Assignment 2</a></li>
                    <li class="current"><a href="assignment3.html">Assignment 3</a></li>
                    <li><a href="assignment4.html">Assignment 4</a></li>
                    <li><a href="assignment5.html">Assignment 5</a></li>
                    <li><a href="blender.html">Blender</a></li>
            </ul>
            <ul class="reg">
                <li><a href="login.html">Login</a></li>
                <li><a href="signup.html">Signup</a></li>
            </ul>
        </nav>
        <div class="sidebar">
            <p>Navigate</p>
            <ul class="sidedrop">
                <li><a href="#navbar">Top</a></li>
                <li><a href="#one">Introduction</a></li>
                <li><a href="#two">C-36</a></li>
                <li><a href="#three">Model</a></li>
                <li><a href="#four">Why?</a></li>
                <li><a href="#five">How?</a></li>
                <li><a href="#six">Conclusion</a></li>
                <li style="border-bottom:5px;"><a href="#bottom">Bottom</a></li>
            </ul>
        </div>
        <div class="content">
        <div class="a3">
        <div class="banner">
            <h1>How can the Government Regulate Social Media?</h1>
            </div>
            <h2>List of acronyms and abbreviations</h2>
            <p>
                <strong>AI</strong><br>Artifical Intelligence<br><strong>CIGI</strong><br>Centre for the International Governance Innovation<br>
                <strong>Ads</strong><br>Advertisements<br><strong>Psych</strong><br>Psychologist
            </p>
     
            <h2 id="one">Introduction</h2>
            <p>
             The rise of social media platforms is one of the defining characteristics of the 21st century. With a steadily increasing userbase of around 4.5 billion active social media users, they have become an essential part of our personal and professional lives. With millions of user-generated content such as photos, videos and blogs being posted every day on various platforms, it has become a tremendous nuisance for governments to maintain and regulate the types of content being uploaded and spread through the country.
             <br>
             One of the main appeal social media is the ability to reach millions of people in an instant. Moreover, many social media platforms allow users to be completely anonymous, which gives the users a lot of freedom and a feeling that there are no real-life consequences to their actions online. <a href="https://www.livemint.com/opinion/columns/anonymity-on-social-media-and-its-ugly-consequences-11612977615856.html">A study conducted by Philip Zimbardo</a>, an American <abbr title ="Psychologist">psych</abbr>. has shown that people when offered a sense of anonymity tends to express more undesirable behaviors compared to their normal selves. Therefore, as a consequence, many people have a completely different and sometimes malicious personality online (Dominic, 2021).  This along with encryption that some platforms provide makes it very easy for illegal and harmful content to spread under the radar of governments. 
            </p>
     
     
            <h2 id="two">Bill C-36</h2>
            <p>
             There are many methods that social media companies can implement to help the governments to regulate the content in their country, but this comes with a significant cost to privacy and security of the platform and the freedom of expression of the people. For instance, many Canadians fear that the Bill C-36, that is currently in the early stages of the legislative process in Canadian Parliament would greatly hinder their freedom of expression, which is one of their Constitutional Rights. The bill aims to reduce hate speech and the spread of harmful contents online by making it easier to punish those who do so. 
     
            <br>
             According to the bill, the government would create a new Digital Safety Commission of Canada. This commission would have the power to regulate hateful and harmful online content such as:
            </p>
             <ul>
                 <li>terrorist content,</li>
                 <li>content that incites violence,</li>
                 <li>hate speech,</li>
                 <li>non-consensual sharing of intimate images, and</li>
                 <li>child sexual exploitation content</li>
             </ul>
             
             <p>
             across various platforms like Facebook, Instagram, Twitter, YouTube, TikTok, Reddit, etc. Moreover, these platforms would be <strong>required to remove user-flagged content within 24 hours</strong> that meets the legislated definitions or face severe consequences.
            <br>
             The bill looks promising on paper, but it could lead to many issues such as limiting speech removal of legitimate content, <a href="https://canada.googleblog.com/2021/11/our-shared-responsibility-youtubes.html">according to Google.</a> Furthermore, the company states that, 
            </p>
            <blockquote cite="https://canada.googleblog.com/2021/11/our-shared-responsibility-youtubes.html">
             "It’s essential to strike the right balance between speed and accuracy. User flags are best utilized as “signals” of potentially violative content, rather than definitive statements of violations. In Q2 2021, 17.2M videos were flagged by users. In that same period, we removed over 6.2M videos for violating our Community Guidelines, and of those removed, 296,454 were first flagged by users."

             </blockquote>
             <p>
             As expressed by Google, the time limitation would cripple their flagging system due to the sear amount of content being flagged everyday and would lead to many legitimate content being removed in the rush due to the fear of getting fined. This is true for all platforms as these companies do not have enough manpower to review all the user reported content and <abbr class="acr" title ="Artifical Intelligence">ai</abbr> are not sophisticated enough to handle the task aptly.
             </p>
     
     
             <h2 id="three">Understanding the Business Model</h2>
             <p>
                 To effectively regulate social media platforms, first we should understand their business and revenue models. All most all major social platforms are completely free for the user and relies on displaying <abbr title="Advertisements">ads</abbr> and collecting user data as their primary means of revenue. The more time a user spends on their platform, the more they can display <abbr title="Advertisements">ads</abbr> and collect data. Therefore, their business model focuses on maximizing user engagement. 
             <br>
                 Many studies have shown that people tend to share content that they feel personal connection or relate to, or the ones that disturbs or creates fear in them. This has led social media algorithms to promote a lot of extreme content to their users that would keeps them engaged and are more likely to share to their friends. Hate speech, content that incites violence against minorities and various types of misinformation that solidifies a user’s preexisting beliefs are some of the common types of content are constantly pushed to specific users based on their biases by the algorithms. <a href="https://www.cigionline.org/articles/resetting-the-debate-on-regulating-social-media/">Haggart and Tusikov from <abbr class="acr" title = "Centre for the International Governance Innovation">cigi</abbr> notes that:</a>
             </p>
             <blockquote cite="https://www.cigionline.org/articles/resetting-the-debate-on-regulating-social-media/">
                 
                     “While social media companies regularly condemn hate speech and disavow violent extremism, until governments dismantle business models that monetize hate, violence and misinformation, regulatory efforts will remain largely ineffective.”
                 
             </blockquote>
     
     
             <h2 id="four">Why Regulate?</h2>
             <p>
                 While some might argue that government involvement in social media platforms will limit free speech, but for the reasons stated above it is of uttermost importance that governments regulate these platforms. Without proper regulation, these platforms would be overrun with misinformation and hate speech. This is evident from fact that many internet content creators, journalists, and activists are forced to leave social media because of discrimination, misogynistic comments, racism from users.
            <br>
                 Canada has around 31.8 million social media users and with a social network penetration rate of 83%, it is one of highest penetration rate across the world (<a href="https://www.statista.com/statistics/247737/top-social-media-sites-visit-share-canada/">Statista Research Department</a>, 2021). Furthermore, all the major social networks in Canada are foreign companies, with most based in the US and abiding their law. Apart from the major privacy and security concerns for the country, this has made it harder for the government to implement certain regulations that are different from the ones specified in the US law. 
             </p>
             <table>
                 <caption>Leading social media websites in Canada in July 2021, based on share of visits</caption>
                 <tbody>
                 <tr>
                     <td>Facebook</td>
                     <td>Pinterest</td>
                     <td>Twitter</td>
                     <td>Instagram</td>
                     <td>Reddit</td>
                     <td>Youtube</td>
                     <td>Tumblr</td>
                     <td>Other</td>
                 </tr>
                 <tr>
                     <td>59.37%</td>
                     <td>14.64%</td>
                     <td>13.09%</td>
                     <td>5.27%</td>
                     <td>2.76%</td>
                     <td>2.42%</td>
                     <td>1.49%</td>
                     <td>0.94%</td>
                 </tr>
                 </tbody>
                 <tfoot>
                     <tr>
                     <td colspan="8">Source: <a href="https://www.statista.com/statistics/247737/top-social-media-sites-visit-share-canada/">Statista Research Department</a></td>
                     </tr>
                 </tfoot>
             </table>
             <h2 id="five">What can be done?</h2>
             <p>
                 All major social media platforms have already implemented features for users to report inappropriate content on their respective platforms. But as mentioned above, it takes a tremendous amount of time and money to investigate these claims and differentiate between legal and illegal content. This is further worsened by internet trolls who simply mass report content either for fun or to bully other internet users. With no real consequences to <em>troll reporting</em> content, these users could easily make an already difficult task even worse. The governments regulations such the Bill C-36 are basically asking these companies to do what they are already doing, but just more of it (Haggart & Tusikov, 2021) and this does not do much to solve the underlying issue.
             <br>
                 There are multiple methods that governments and social media companies can implement to better regulate the online content, but all of them come with some consequence to an individual’s security, privacy, or freedom of expression. Some rules and regulations that could be feasibility, but not perfectly implement are:
             </p>
     
     
             <h3>Method 1: Government ID verification on social media</h3>
             <p>
                 Although controversial and highly questionable, this method could solve a number of issues plaguing these social networks such as underaged children accessing these platforms and would most definitely make people thing twice before posting illegal content. This would also make it easier for law enforcement to investigate and arrest online perpetrators. But this method has some major disadvantages to the users and the social media platforms. Giving a government ID to companies such as Facebook, Google, TikTok could be disastrous. These companies would be able to create a very detailed profile of very user across various platforms and completely ruin a user’s anonymity, for better or worse. One way to circumnavigate this is by creating a government login portal separated from the social networks, where the user would enter their government issued identification number during the sign-up process to the platform. This would ensure that these social companies cannot track users across platforms.
             <br>
                 But of course, this brings up a concern about governments being able to track a user’s every move across various platforms. This can be detrimental for those who go against the government especially if corruption occurs within the government. This concern can be reduced by encrypting and storing a user’s data in a very secure manner and making it very difficult for authorities and government officials to access the data without a proper cause. But the biggest risk using this system is when a data break occurs. Cyber criminals could make the information public and people could be easily doxed across all connected platforms.
             </p>
     
             
             <h3>Method 2: Support from Government</h3>
             <p>
                 Social media is vital for every country’s development, so it makes sense that governments help these companies to improves technologies such as security, better content moderation algorithms, etc. that could better protect the country by providing funding. With the extra funding these companies could create better AIs to moderate connect and hire more people to moderate content locally in each country according to the funding they receive from them. Governments could also create a system where people would be rewarded for reporting highly illegal online activities, this would highly encourage social media users to moderate content. 
             </p>
     
             <h3>Method 3: Better Reporting Features</h3>
             <p>
                 On the topic moderation, social media platforms could also create a priority reporting system along with the usual reporting feature where the user could pay a small amount to the social network to investigate their case quicker and then return the paid amount if the content they reported is actually illegal. This would help to take down serious content such as non-consensual sharing of intimate images, hate speech and child sexual exploitation content quickly. This way moderators would be able to prioritize which reported content to check first and the small fee would greatly discourage internet trolls from using this system.
             </p>
     
             
             <h2 id="six">Conclusion</h2>
             <p>
                 The methods 2 and 3 would not be as effective as method 1, but they not as radical as it either. With billions of users uploading millions of content every day on hundreds of platforms, it is next to impossible to moderate every single user-generated content. This is a monumental undertaking for both the governments and social media companies. Every solution to this problem always comes with its own set of drawbacks. This problem is further worsened by the fact that most governments across the world have vastly different rules and are mostly run by people who do not know how these platforms operate.
             </p>
             
             <div class="References">
             <h5>References</h5>
                 <ul>
                     <li>Dominic, B. (2021, February 10). Live Mint. <a href="https://www.livemint.com/opinion/columns/anonymity-on-social-media-and-its-ugly-consequences-11612977615856.html"><cite> on social media and its ugly consequences</cite></a></li>
                     <li>Google Canada. (2021, November 5). <a href="https://canada.googleblog.com/2021/11/our-shared-responsibility-youtubes.html"><cite>Our Shared Responsibility: YouTube’s response to the Government’s proposal to address harmful content online</cite></a></li>
                     <li>Haggart, B. & Tusikov, N. (2021, September 8). <a href="https://www.cigionline.org/articles/resetting-the-debate-on-regulating-social-media/"><cite>Resetting the Debate on Regulating Social Media: Part One</cite></a></li>
                     <li>Statista Research Department. (2021, July). <a href="https://www.statista.com/statistics/247737/top-social-media-sites-visit-share-canada/"><cite>Leading social media websites in Canada in July 2021, based on share of visits</cite></a></li>
                 </ul>
             </div>
        </div>
        </div>

    </body>


    <footer id="bottom">
        <p class="name">Athul Subash Marottikkal</p>
        <div class="icons">
        
        <a href="#" class="fa fa-facebook"></a>
        <a href="#" class="fa fa-instagram"></a>
        <a href="#" class="fa fa-google"></a>
        <a href="#" class="fa fa-twitter"></a>
        <a href="#" class="fa fa-linkedin"></a>       
        </div>
        <p class="foot"><a href="index.html">Home</a> | <a href="">Contact us</a> </p>
        <p class="notice">Copyright &copy; 2021 </p>
        
    </footer>
</html>